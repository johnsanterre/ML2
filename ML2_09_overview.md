# Week 9: From Supervised to Generative Learning

## Course Overview

This week explores the transition from traditional supervised learning to modern generative approaches, with a focus on diffusion models and self-supervised learning techniques.

## Learning Objectives

By the end of this week, students will be able to:
- Understand the transition from supervised to generative approaches
- Master the principles of diffusion models
- Grasp self-supervised learning techniques
- Connect diffusion concepts across modalities

## Topics Covered

### 1. Beyond Supervised Learning
- Limitations of labeled data
    * Cost of annotation
    * Expert knowledge requirements
    * Scale limitations
- The generative alternative
    * Learning data distributions
    * Self-supervised learning
    * Implicit vs explicit modeling

### 2. Diffusion Models: Core Concepts
- Forward process
    * Noise scheduling
    * Gradual information destruction
    * Markov chains
- Reverse process
    * Denoising steps
    * Score matching
    * Time conditioning
- Applications and use cases
- Implementation considerations

### 3. Self-Supervised Learning
- Masked prediction tasks
    * BERT-style masking
    * Image patch prediction
    * Corruption and reconstruction
- Contrastive learning approaches
    * Positive/negative pairs
    * SimCLR approach
    * Momentum contrast
- Connection to diffusion models
    * Denoising as self-supervision
    * Learning without labels
    * Representation quality

## Required Reading
- "Denoising Diffusion Probabilistic Models" (Ho et al.)
- "Understanding Diffusion Models: A Unified Perspective"

## Practical Exercises
1. Implementing basic diffusion models
2. Exploring self-supervised learning techniques
3. Comparing supervised and generative approaches

## Assessment
- Weekly Quiz (20%)
- Programming Assignment (40%)
- Theoretical Exercise (40%)

## Additional Resources
- Research papers on diffusion models
- Implementation tutorials
- Case studies in generative learning 
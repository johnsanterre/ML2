# Week 12: Retrieval Augmented Generation (RAG)

## 1. Foundations of RAG

### 1.1 Beyond Model Knowledge
## Limitations of Large Language Models and the Need for Retrieval-Augmented Generation

Large language models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks, showcasing their ability to generate coherent and contextually relevant text. However, despite their impressive capabilities, LLMs face inherent limitations in their pretrained knowledge. These limitations stem from the static nature of the model weights, which are fixed after the pretraining phase. In contrast, human knowledge is dynamic, constantly evolving, and expanding over time. This discrepancy between the static nature of LLMs and the dynamic nature of knowledge poses significant challenges in terms of temporal boundaries, domain specificity, and factual accuracy.

One of the primary limitations of LLMs is their temporal boundaries. During the pretraining phase, LLMs are exposed to a vast corpus of text data up to a certain cutoff date. This means that any knowledge or information that emerges after the cutoff date is not captured by the model. As a result, LLMs may struggle to generate accurate and up-to-date responses to queries that involve recent events, discoveries, or advancements. For example, an LLM trained on data up to 2020 may not have knowledge about the latest scientific breakthroughs, political developments, or social trends that have occurred since then. This temporal limitation can lead to information gaps and outdated responses, which can be problematic in domains that require the most current and relevant information.

Another limitation of LLMs is their domain specificity. While LLMs are trained on a diverse range of text data, they may lack the depth and granularity of knowledge required in highly specialized fields. For instance, an LLM trained on general web data may have a broad understanding of medical terminology but may struggle to provide accurate and nuanced responses to complex medical queries. Similarly, in domains such as law, finance, or engineering, LLMs may not possess the level of expertise and domain-specific knowledge that human experts have acquired through years of specialized training and experience. This limitation highlights the need for domain-specific fine-tuning or the incorporation of external knowledge sources to enhance the performance of LLMs in specialized domains.

Furthermore, LLMs are susceptible to factual inaccuracies and hallucinations. Hallucinations refer to the generation of plausible but incorrect information by the model. This can occur when the model conflates different pieces of information or makes assumptions based on incomplete or ambiguous input. LLMs may also struggle to distinguish between factual statements and opinions, leading to the generation of responses that may sound convincing but lack factual accuracy. These issues arise due to the inherent uncertainty in the pretraining data and the model's inability to verify the truthfulness of the information it generates. Ensuring factual accuracy is crucial in many applications, such as question answering, information retrieval, and decision support systems, where the reliability and trustworthiness of the generated responses are of utmost importance.

To address these limitations, researchers have proposed the development of retrieval-augmented generation (RAG) systems. RAG systems combine the generative capabilities of LLMs with the ability to dynamically access and incorporate external knowledge. The key idea behind RAG is to augment the LLM with a retrieval mechanism that allows it to search and retrieve relevant information from external knowledge sources, such as databases, documents, or the internet. By integrating this retrieved knowledge into the generation process, RAG systems can provide more accurate, up-to-date, and domain-specific responses. The retrieval mechanism can be based on various techniques, such as dense vector retrieval, sparse vector retrieval, or semantic search, depending on the nature of the knowledge sources and the specific requirements of the task. RAG systems have shown promising results in tasks such as open-domain question answering, fact verification, and knowledge-intensive language generation, demonstrating their potential to overcome the limitations of standalone LLMs.
### 1.2 RAG Architecture

The RAG architecture comprises two primary components working in concert:

## Retriever-Generator Architecture in Question Answering Systems

The retriever-generator architecture is a powerful approach for building question answering systems that can effectively handle a wide range of queries. This architecture consists of two main components: the retriever and the generator. The retriever is responsible for finding the most relevant documents or passages from a large corpus based on the input query, while the generator uses the retrieved context to generate a coherent and accurate answer.

The retriever component can be mathematically formulated as:

$$R(q) = \text{argmax}_{d \in D} \text{sim}(e_q, e_d)$$

where \(q\) represents the input query, \(D\) is the document collection, \(e_q\) and \(e_d\) are the embeddings of the query and documents, respectively, and \(\text{sim}\) is a similarity function, often chosen to be cosine similarity. The retriever aims to find the document \(d\) that maximizes the similarity between its embedding \(e_d\) and the query embedding \(e_q\). This process allows the system to identify the most relevant information for answering the query.

Once the retriever has selected the most relevant context \(c\), the generator component takes over to produce a natural language answer. The generator is typically a language model that estimates the conditional probability distribution of the output tokens given the input query and the retrieved context. Mathematically, the generator can be formalized as:

$$G(q, c) = \prod_{i=1}^n P(w_i|w_{<i}, q, c)$$

where \(w_i\) are the output tokens, and \(P\) is the conditional probability distribution. The generator sequentially predicts each token \(w_i\) based on the previously generated tokens \(w_{<i}\), the input query \(q\), and the retrieved context \(c\). By conditioning on both the query and the context, the generator can produce answers that are not only fluent and grammatically correct but also relevant and informative.

The retriever-generator architecture has several advantages over other approaches to question answering. First, by separating the retrieval and generation tasks, the system can handle a much larger knowledge base than methods that rely solely on generation. Second, the retriever can be optimized independently of the generator, allowing for more efficient search and retrieval techniques. Finally, the generator can focus on producing high-quality answers without being burdened by the task of searching through a large corpus.
## 1.3 Vector Databases

Vector databases are designed to efficiently store and retrieve high-dimensional vectors, enabling fast similarity search and nearest neighbor queries. These databases leverage specialized index structures and algorithms to optimize the search process, making them well-suited for applications such as recommendation systems, image retrieval, and natural language processing.

One of the key components of vector databases is the use of Approximate Nearest Neighbor (ANN) algorithms. These algorithms aim to find the most similar vectors to a given query vector in sub-linear time complexity. Two popular ANN algorithms are Hierarchical Navigable Small World (HNSW) and Inverted File (IVF). HNSW achieves a query complexity of \[O(\log(n))\], where \(n\) is the number of vectors in the database. It constructs a hierarchical graph structure that allows for efficient navigation and search. On the other hand, IVF employs clustering techniques to partition the vector space and achieves a complexity of \[O(\sqrt{n})\]. By dividing the vectors into clusters, IVF can quickly narrow down the search space and find the nearest neighbors within the relevant clusters.

In addition to ANN algorithms, vector databases utilize specialized index structures to organize and partition the high-dimensional vector space. One such structure is the Voronoi diagram, which divides the space into regions based on the proximity of vectors to their nearest neighbors. Each region in the Voronoi diagram represents the set of points that are closer to a particular vector than to any other vector in the database. This space partitioning allows for efficient pruning of the search space during similarity queries. Another index structure commonly used in vector databases is Locality-Sensitive Hashing (LSH). LSH is a technique that maps high-dimensional vectors to lower-dimensional hash codes, preserving the locality of similar vectors. By hashing similar vectors to the same or nearby hash buckets, LSH enables fast approximate similarity search by reducing the dimensionality of the search space.

The combination of ANN algorithms and specialized index structures enables vector databases to handle large-scale datasets and perform fast similarity search. When a query vector is provided, the database uses the index structures to quickly identify the most promising regions or clusters to search within. The ANN algorithms then refine the search within these regions to find the nearest neighbors. This two-step approach significantly reduces the computational overhead compared to exhaustive search methods, making vector databases scalable and efficient.

Vector databases have found numerous applications across various domains. In recommendation systems, vector databases can be used to find similar items or users based on their feature vectors, enabling personalized recommendations. In image retrieval, vector databases can efficiently search for visually similar images by comparing their feature vectors. Similarly, in natural language processing, vector databases can be used to find semantically similar documents or perform semantic search based on word embeddings or document vectors. The ability to perform fast and accurate similarity search has made vector databases an essential tool in many data-driven applications.

## ## 2.1 Document Processing

The first step in building a Retrieval-Augmented Generation (RAG) system is to process the documents that will serve as the knowledge base. This involves breaking down the documents into smaller, manageable chunks and generating embeddings for each chunk. The choice of chunking strategy depends on the nature of the documents and the desired granularity of retrieval. Fixed-size chunking with overlap is a simple approach, where the text is divided into chunks of size $k$ with an overlap of $\delta$ tokens between consecutive chunks. This ensures that relevant information is not lost due to arbitrary chunk boundaries. The chunk size and overlap can be tuned based on the average length of the documents and the expected query complexity.

Semantic chunking leverages discourse boundaries, such as paragraphs or sections, to create more coherent and self-contained chunks. This approach relies on the inherent structure of the documents and assumes that the author has organized the content in a meaningful way. Semantic chunking can lead to more accurate retrieval, as the chunks are likely to contain complete ideas or concepts. However, it may result in chunks of varying sizes, which can affect the efficiency of the retrieval process. To address this, hierarchical chunking can be employed, where the documents are chunked at multiple levels of granularity, such as sentences, paragraphs, and sections. This allows for flexible retrieval based on the complexity of the query and the desired level of detail in the response.

Once the documents are chunked, the next step is to generate embeddings for each chunk. Embeddings are dense, fixed-dimensional vector representations that capture the semantic meaning of the text. The embedding function $f_\theta(x)$ maps a chunk $x$ to a vector $e \in \mathbb{R}^d$, where $d$ is the dimensionality of the embedding space. The choice of embedding function depends on the specific requirements of the RAG system, such as the desired level of contextualization and the computational resources available. Popular embedding models include word2vec, GloVe, and BERT, which can be fine-tuned on the specific domain or task.

In addition to dense embeddings, cross-encoders can be used to generate more expressive representations that capture the interaction between the query and the document. A cross-encoder $g_\phi(q, d)$ takes the query $q$ and a document chunk $d$ as input and produces a scalar score $s$ that represents the relevance of the chunk to the query. Cross-encoders are more computationally expensive than dense embeddings, as they require processing the query and the document together for each chunk. However, they can provide more accurate retrieval results, especially for complex queries that require a deeper understanding of the context.

The generated embeddings are then indexed for efficient retrieval during the query processing stage. The choice of indexing method depends on the size of the knowledge base and the desired retrieval speed. Inverted indexes, such as Lucene or Elasticsearch, are commonly used for large-scale retrieval, while approximate nearest neighbor (ANN) methods, such as locality-sensitive hashing (LSH) or hierarchical navigable small world (HNSW), are employed for fast similarity search in high-dimensional spaces. The indexed embeddings serve as the foundation for the retrieval component of the RAG system, enabling efficient access to relevant knowledge based on the input query.
### 2.2 Retrieval Strategies
## Dense Retrieval and Hybrid Search in Information Retrieval Systems

Dense retrieval is a powerful technique in modern information retrieval systems that enables efficient and effective similarity search between query and document representations. The core idea behind dense retrieval is to learn low-dimensional, dense vector representations for both queries and documents, such that the similarity between them can be computed using simple vector operations. One of the most commonly used similarity measures in dense retrieval is the Maximum Inner Product Search (MIPS), which calculates the cosine similarity between the query vector $q$ and the document vector $d$ as follows:

$$\text{sim}(q, d) = \frac{q^T d}{\|q\| \|d\|}$$

Here, $q^T$ denotes the transpose of the query vector, and $\|q\|$ and $\|d\|$ represent the Euclidean norms of the query and document vectors, respectively. By maximizing the inner product between the query and document vectors, MIPS effectively identifies the most relevant documents for a given query based on their semantic similarity in the dense vector space.

While dense retrieval has shown impressive performance in various information retrieval tasks, it is not always sufficient on its own, especially when dealing with complex and diverse information needs. To address this limitation, researchers have proposed hybrid search approaches that combine the strengths of dense retrieval with traditional sparse retrieval techniques, such as term-based inverted indexes. Hybrid search aims to leverage the semantic understanding captured by dense representations while also exploiting the exact keyword matching capabilities of sparse retrieval methods.

One common approach to hybrid search is the use of ensemble methods, where the final relevance score of a document is computed as a weighted combination of the scores obtained from dense and sparse retrieval systems. Mathematically, this can be expressed as:

$$s_\text{final} = \alpha s_\text{dense} + (1-\alpha) s_\text{sparse}$$

In this equation, $s_\text{final}$ represents the final relevance score, $s_\text{dense}$ and $s_\text{sparse}$ denote the scores from the dense and sparse retrieval systems, respectively, and $\alpha$ is a hyperparameter that controls the relative importance of each component. By tuning the value of $\alpha$, the hybrid search system can effectively balance the contributions of dense and sparse retrieval based on the specific characteristics of the retrieval task and the available data.

Another popular approach to hybrid search is the use of late fusion techniques, where the results from dense and sparse retrieval systems are combined at a later stage in the retrieval pipeline. Late fusion can be performed using various strategies, such as rank aggregation, score aggregation, or learning-to-rank methods. The goal of late fusion is to leverage the complementary information provided by dense and sparse retrieval to generate a more accurate and comprehensive ranked list of relevant documents.

In recent years, cross-attention mechanisms have emerged as a promising technique for hybrid search, particularly in the context of transformer-based models like BERT and its variants. Cross-attention allows the model to attend to both the query and document representations simultaneously, enabling a more fine-grained and context-aware matching between them. By incorporating cross-attention into the hybrid search framework, the system can effectively capture the interactions between the query and document at various granularities, leading to improved retrieval performance.

### 2.3 Context Integration
## Prompt Engineering

Prompt engineering is a crucial aspect of developing effective and efficient language models in the field of data science. It involves the meticulous crafting of input prompts to guide the model towards generating desired outputs. The primary objective of prompt engineering is to optimize the interaction between the user and the language model, ensuring that the model comprehends the user's intent and provides accurate and relevant responses.

One fundamental approach to prompt engineering is template-based prompting. This method involves constructing a structured template that encapsulates the essential components of the user's query. The template typically includes placeholders for the retrieved documents or context relevant to the query, as well as the specific question posed by the user. By adhering to a consistent template format, the language model can effectively parse the input and focus on generating an appropriate response based on the provided context. The template-based approach enables the model to maintain a clear separation between the context and the question, facilitating more accurate and coherent answers.

Another key technique in prompt engineering is the utilization of few-shot exemplars. Few-shot learning refers to the ability of a language model to learn from a limited number of examples or demonstrations. By providing the model with a small set of representative examples that showcase the desired input-output behavior, the model can quickly adapt and generate similar responses for new queries. Few-shot exemplars serve as a form of conditioning, guiding the model towards the expected format and style of the generated output. This approach is particularly valuable when dealing with domain-specific or specialized tasks, where the availability of large-scale training data may be limited.

Chain-of-thought reasoning is an advanced prompt engineering technique that aims to improve the model's ability to perform multi-step reasoning and generate more coherent and logical responses. In this approach, the input prompt is structured to encourage the model to break down the problem into a series of intermediate steps or sub-questions. By explicitly guiding the model through a step-by-step thought process, chain-of-thought reasoning enables the model to arrive at the final answer through a sequence of logical deductions. This technique is especially beneficial for complex tasks that require multiple reasoning steps, such as mathematical problem-solving or answering questions that involve causal relationships.

The effectiveness of prompt engineering relies on a deep understanding of the language model's architecture, training data, and intended use case. Data scientists must carefully design prompts that align with the model's capabilities and the specific task at hand. This involves considering factors such as the complexity of the query, the available context, and the desired output format. Additionally, iterative refinement and experimentation are essential to optimize the prompts and improve the model's performance over time. By continuously evaluating the model's responses and incorporating feedback, data scientists can fine-tune the prompts to achieve higher accuracy and generate more coherent and contextually relevant outputs.

### 3.1 Quality Control
## Relevance Assessment and Fact Verification in Information Retrieval

Relevance assessment is a crucial component of information retrieval systems, as it determines the effectiveness of the system in providing users with the most relevant and accurate information. Quantitative metrics such as Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR) are commonly used to evaluate the performance of relevance assessment algorithms. NDCG measures the usefulness of a document based on its position in the result list, while MRR focuses on the rank of the first relevant document. These metrics provide a standardized way to compare the performance of different relevance assessment techniques and help identify areas for improvement.

Ground truth comparison is another essential aspect of relevance assessment, where the results obtained from the algorithm are compared against a set of manually annotated relevant documents. This process helps to validate the accuracy of the relevance assessment algorithm and identify any discrepancies between the algorithm's output and human judgment. Ground truth comparison is particularly useful in scenarios where the relevance of documents is subjective or context-dependent, as it allows for a more nuanced evaluation of the system's performance.

Human-in-the-loop evaluation is an approach that incorporates human feedback into the relevance assessment process. This method involves presenting users with a set of search results and asking them to provide feedback on the relevance of each document. The feedback is then used to refine the relevance assessment algorithm, allowing it to better capture user preferences and context-specific relevance criteria. Human-in-the-loop evaluation is particularly valuable in domains where the relevance of information is highly subjective, such as personalized recommendation systems or domain-specific search engines.

Fact verification is another critical aspect of information retrieval, as it ensures that the information provided to users is accurate and trustworthy. Source triangulation is a common technique used for fact verification, where information is cross-referenced across multiple reliable sources to confirm its validity. This approach helps to identify and filter out false or misleading information, improving the overall quality of the retrieved information.

Confidence scoring is a technique used to assign a measure of certainty to the verified facts. The confidence score is typically a function of individual relevance scores, denoted as:

$$c = f(r_1, r_2, ..., r_n)$$

where $r_i$ represents the relevance score of the $i$-th source. The confidence scoring function $f$ can be designed to take into account various factors, such as the reliability of the sources, the consistency of the information across sources, and the overall relevance of the information to the user's query. By assigning confidence scores to verified facts, the information retrieval system can provide users with a measure of the trustworthiness of the information, allowing them to make more informed decisions based on the retrieved results.
## References


## Exercises

1. Implement a basic RAG system using FAISS and analyze its retrieval performance.
2. Compare different chunking strategies and their impact on retrieval quality.
3. Design and implement a caching strategy for a RAG system handling high query volumes.
4. Develop a hybrid search system combining dense and sparse retrievers. 

Adendum:


## Information Retrieval in RAG Systems

Information retrieval (IR) is a critical component of Retrieval-Augmented Generation (RAG) systems, which aim to generate coherent and informative text by leveraging external knowledge sources. In RAG systems, the IR module is responsible for efficiently searching and retrieving relevant information from a large corpus of documents, such as Wikipedia articles or domain-specific databases. The retrieved information is then used to augment the input context and guide the text generation process.

The IR module in RAG systems typically employs various techniques to identify and rank the most relevant documents based on the input query. One common approach is to use a dense retrieval model, such as Dense Passage Retrieval (DPR) or Contrastive Representation Learning (CRL). These models learn to map both the query and the document passages into a shared embedding space, where semantically similar queries and passages are close to each other. The relevance score between a query and a passage is then computed using a similarity metric, such as cosine similarity or dot product. The top-k most relevant passages are retrieved and used as additional context for the text generation module.

Another important aspect of IR in RAG systems is the efficient indexing and search of the document corpus. Given the large scale of the corpus, it is crucial to have a fast and scalable indexing mechanism that allows for real-time retrieval. Inverted indexes, such as Elasticsearch or Lucene, are commonly used for this purpose. These indexes store a mapping between each unique term in the corpus and the documents that contain it, enabling quick lookup of relevant documents based on the query terms. Additionally, techniques like term frequency-inverse document frequency (TF-IDF) weighting and BM25 scoring are employed to prioritize documents that contain rare and informative terms.

The retrieved information from the IR module is then integrated with the input context to form an augmented context for the text generation module. This augmented context provides additional knowledge and context that can help the model generate more accurate and informative responses. The integration of the retrieved information can be done in various ways, such as concatenating the retrieved passages with the input context, using attention mechanisms to selectively attend to relevant parts of the retrieved information, or employing graph-based representations to capture the relationships between the input context and the retrieved passages.

The effectiveness of the IR module in RAG systems is evaluated using various metrics, such as recall@k, which measures the proportion of relevant documents retrieved within the top-k results, and mean reciprocal rank (MRR), which assesses the average reciprocal rank of the first relevant document in the retrieved results. These metrics help quantify the ability of the IR module to retrieve relevant information and provide insights into the overall performance of the RAG system. Improving the IR module, through better retrieval models, indexing techniques, or query expansion strategies, can significantly enhance the quality of the generated text and the overall performance of the RAG system.
# ML1 Week 12: Dimensional Transformations: From SVMs to Deep Learning

## Overview
This week explores how different ML approaches transform data into higher dimensions to solve complex problems. Starting with SVMs and progressing toward deep learning concepts, we examine how dimensional transformations enable powerful classification capabilities.

## Learning Objectives
By the end of this session, students will:
- Understand dimensional transformation in ML
- Connect SVM concepts to deep learning intuitions
- Visualize higher-dimensional mappings
- Grasp the power of learned transformations
- Bridge traditional ML and neural approaches

## Topics Covered

### 1. Support Vector Machines as Transformers
- Conceptual Framework
  * Linear separation limitations
  * Higher dimensional projections
  * Kernel transformation intuition
  * Feature space mapping

### 2. From Explicit to Learned Mappings
- Traditional Approaches
  * Fixed transformations
  * Feature engineering
  * Geometric constraints
- Transition Concepts
  * Learned features
  * Adaptive transformations
  * Hierarchical representations
  * Automatic feature discovery

### 3. Deep Learning Transformations
- Neural Perspectives
  * Layer-wise transformations
  * Nonlinear projections
  * Dimensional manipulation
  * Learned representations
- Visual Understanding
  * 2D to 3D mappings
  * Manifold transformations
  * Feature space warping
  * Classification boundaries


## Key Takeaways
1. Higher dimensions enable complex separations
2. SVMs and deep learning share transformation concepts
3. Learning transformations adds power and flexibility
4. Geometric intuitions unite different approaches

## Practical Exercises
1. Visualize dimensional transformations
2. Compare SVM and neural mappings
3. Explore learned representations
4. Analyze transformation effects 
# Deep Learning to LLMs: Theory and Application

## Course Overview
This course transitions from deep learning fundamentals through to practical LLM applications, providing both theoretical understanding and hands-on experience.

## Prerequisites
- Basic Python programming
- Understanding of linear algebra and calculus
- Familiarity with machine learning concepts

## Course Structure

### Part 0: Foundations (Weeks 0.01-0.02)
- **0.01: Python Fundamentals - From Scripts to Classes**
  - Converting scripts to functions
  - Object-oriented programming basics
  - Best practices in code organization

- **0.02: From Basic Probability to High Dimensions**
  - Basic probability and Bayes' Theorem
  - Computational limitations
  - Curse of dimensionality
  - P-values and multiple testing

### Part 1: Deep Learning Foundations (Weeks 1-6)
- **Week 1: Introduction to Neural Networks**
  - Basic neural network concepts
  - Forward and backward propagation
  - Loss functions and optimization

- **Week 2: Neural Networks Fundamentals & Backpropagation**
  - Detailed backpropagation
  - Gradient descent variations
  - Training challenges

- **Week 3: Building Real-World Models**
  - California Housing Dataset implementation
  - Model development pipeline
  - Performance optimization

- **Week 4: Vector Representations & Similarity**
  - Vector representations
  - Similarity measures
  - Feature engineering
  - Representation learning

- **Week 5: Autoencoders & Embeddings**
  - Autoencoder architectures
  - Embedding spaces
  - Dimensionality reduction
  - Feature learning

- **Week 6: From Autoencoders to Embeddings**
  - Word embeddings
  - Multi-modal embeddings
  - Training strategies
  - Practical applications

### Part 2: Advanced Architectures (Weeks 7-8)
- **Week 7: Introduction to Transformers**
  - Self-attention mechanism
  - Transformer architecture
  - Positional encoding
  - Multi-head attention

- **Week 8: Convolutional Neural Networks**
  - CNN fundamentals
  - Architecture components
  - Modern CNN designs
  - Practical applications

### Part 3: Modern Language Models (Weeks 9-13)
- **Week 9: From Supervised to Generative Learning**
  - Limitations of supervised learning
  - Generative approaches
  - Diffusion models
  - Self-supervised learning

- **Week 10: Introduction to Large Language Models**
  - Evolution of language models
  - Pre-training concepts
  - Model capabilities
  - Limitations

- **Week 11: Practical LLM Integration & API Development**
  - API integration
  - Prompt engineering
  - Error handling
  - Production deployment

- **Week 12: Retrieval Augmented Generation (RAG)**
  - RAG architecture
  - Document processing
  - Vector databases
  - Implementation strategies

- **Week 13: Evaluating LLMs**
  - Evaluation metrics
  - Benchmark suites
  - Human evaluation
  - Quality assurance

### Special Topics
- **Week 21: From Graph Theory to Deep RL**
  - Network flow problems
  - Ford-Fulkerson algorithm
  - Graph-based RL
  - Modern applications

## Assessment Structure
- Weekly Quizzes (20%)
- Programming Assignments (40%)
- Final Project (30%)
- Class Participation (10%)

## Learning Outcomes
By the end of this course, students will be able to:
1. Implement neural networks from scratch
2. Understand modern deep learning architectures
3. Work with transformer-based models
4. Deploy LLM-based applications
5. Evaluate model performance
6. Build production-ready ML systems

## Required Tools
- Python 3.8+
- PyTorch/TensorFlow
- Jupyter Notebooks
- Git for version control

## Recommended Reading
- Deep Learning Book (Goodfellow et al.)
- Attention Is All You Need (Vaswani et al.)
- LLM-specific papers (listed in weekly materials) 
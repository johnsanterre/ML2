WEEK 10: INTRODUCTION TO LARGE LANGUAGE MODELS

1. Evolution of Language Models
   - Historical perspective
     * N-gram models
     * RNN-based models
     * Transformer revolution
   - Scaling laws
     * Model size trends
     * Compute requirements
     * Data scaling
   - Architecture developments
     * GPT series evolution
     * PaLM architecture
     * Emergent capabilities

2. Pre-training and Foundation Models
   - Training objectives
     * Next token prediction
     * Masked language modeling
     * Causal language modeling
   - Data considerations
     * Web-scale datasets
     * Data quality
     * Filtering strategies
   - Computational challenges
     * Distributed training
     * Memory optimization
     * Training stability

3. Understanding LLM Behavior
   - Model capabilities
     * In-context learning
     * Few-shot learning
     * Zero-shot generalization
   - Internal mechanics
     * Attention patterns
     * Knowledge storage
     * Token representations
   - Limitations
     * Hallucinations
     * Reasoning gaps
     * Bias issues

Required Reading:
- "Language Models are Few-Shot Learners" (GPT-3 paper)
- "On the Opportunities and Risks of Foundation Models"

Learning Objectives:
- Understand the evolution and scaling of language models
- Master key concepts in LLM pre-training
- Grasp emergent behaviors in large models
- Recognize capabilities and limitations 
# ML1 Week 6: Decision Trees and Random Forests

## Overview
This week covers decision trees from fundamentals through advanced concepts, including tree construction, optimization, and ensemble methods. The progression to random forests demonstrates how combining multiple models can leverage the "wisdom of crowds" principle to improve performance.

## Learning Objectives
By the end of this session, students will:
- Understand decision tree fundamentals
- Implement tree construction algorithms
- Master splitting criteria and their derivations
- Apply pruning techniques effectively
- Understand ensemble learning principles
- Apply collective decision-making concepts

## Topics Covered

### 1. Decision Tree Foundations
- Core Concepts
  * Tree structure and terminology
  * Node types and functions
  * Decision boundaries

### 2. Tree Construction Process
- Building Algorithm
  * Recursive splitting
  * Feature selection methods
  * Threshold optimization
- Split Criteria
  * Gini impurity derivation
  * Information gain calculation
  * Entropy-based measures
  * Comparison of methods

### 3. Tree Optimization
- Stopping Criteria
  * Maximum depth rules
  * Minimum sample thresholds
  * Purity requirements
- Pruning Strategies
  * Chi-squared pruning
- Decision Boundaries
  * Boundary visualization
  * Geometric interpretation
  * Boundary optimization

### 4. Tree Generalization
- Model Improvement
  * Overfitting prevention
  * Cross-validation strategies
  * Error estimation
- Ensemble Methods Progression
  * Ensemble principles
  * Wisdom of crowds effect
  * Bagging concepts
  * Feature randomization
  * Diversity in decision making
  * Voting mechanisms
- Advanced Boosting
  * Random Forest to XGBoost
  * Sequential learning
  * Gradient boosting concepts
  * Optimization improvements

## Key Takeaways
1. Decision trees provide interpretable solutions
2. Split criteria choice impacts tree performance
3. Pruning is essential for generalization
4. Collective wisdom improves model performance
5. Diverse ensemble members enhance predictions

## Practical Exercises
1. Implement decision tree from scratch
2. Compare different split criteria
3. Apply various pruning techniques
4. Explore ensemble diversity and performance
5. Compare individual vs collective predictions 
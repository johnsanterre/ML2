# 0.02: From Basic Probability to High Dimensions

## Overview
This session explores why traditional statistical methods, including Bayes' Theorem and P-value calculations, become challenging or impossible to use with high-dimensional data. We'll examine how computational limitations, data sparsity, and multiple comparison problems force us to seek alternative approaches.

## Learning Objectives
By the end of this session, students will:
- Understand basic probability and Bayes' Theorem
- Recognize computational limitations with probability chains
- Understand the curse of dimensionality
- Grasp why P-values become unreliable in high dimensions
- Understand the multiple comparison problem
- Appreciate why we need machine learning approaches

## Topics Covered

### 1. Basic Probability Review
- Prior and posterior probabilities
- Conditional probability
- Bayes' Theorem basics
- Simple chain probabilities
- P-values and statistical significance

### 2. The Multiplication Problem
- Why we multiply probabilities
- What happens with long chains
- Computational underflow
- Practical examples of probability breakdown
- Impact on confidence intervals

### 3. The Data Sparsity Problem
```python
# Example: Word Sequence Probability
# P(cat AND sat AND on AND the AND mat)
# If each word has p = 0.01
p = 0.01 ** 5  # Becomes extremely small
# With 1 million documents, might never see this exact sequence
```

### 4. Curse of Dimensionality
- Why high dimensions are different
- Volume of high-dimensional spaces
- Sparsity of real data
- Why sampling fails
- Multiple comparison problem
- False discovery rate

## Key Concepts

### The Multiple Testing Problem
- Why P-values break down with many comparisons
- Bonferroni correction and its limitations
- False Discovery Rate (FDR)
- Why traditional corrections become too conservative

### The Probability Chain Problem
As we multiply probabilities (especially small ones):
1. Numbers become extremely small
2. Computer precision limits are reached
3. Underflow errors occur
4. Results become meaningless

### The Data Collection Problem
For a sequence of n events, each with m possible values:
- Possible combinations: m^n
- Required data grows exponentially
- Most combinations never observed
- Cannot estimate probabilities reliably

### Real-World Example
Consider language modeling:
- 10,000 common words
- 5-word sequences
- 10,000^5 possible combinations
- Most never appear in any dataset
- Cannot calculate probabilities directly

## Why This Matters

Understanding these limitations helps explain:
1. Why we need neural networks
2. Why embedding spaces work better
3. Why we use continuous representations
4. How modern ML overcomes these limits

## Practice Concepts
- Calculate simple chain probabilities
- Observe numerical underflow
- Experience data sparsity
- Visualize high-dimensional spaces

## Looking Ahead
This understanding provides foundation for:
- Neural network approaches
- Word embeddings
- Dimensionality reduction
- Modern ML architectures 